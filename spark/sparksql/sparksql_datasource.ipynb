{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "来自：examples/src/main/python/sql/datasource.py\n",
    "包含：\n",
    "    1、读取、写入数据（三种方式）\n",
    "    2、根据 parquet file 、json file 等创建临时视图\n",
    "    3、合并 parquet schema\n",
    "    3、连接 mysql\n",
    "                    \n",
    "DataFrame函数：http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"datasource\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n\n+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n\n+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 三种读取方式\n",
    "\n",
    "df1 = spark.read.load(\"data/users.parquet\")\n",
    "df2 = spark.read.parquet(\"data/users.parquet\")\n",
    "df3 = spark.sql(\"select * from parquet.`data/users.parquet`\")\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "df3.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 三种写入方式  当前pyspark==2.4.4\n",
    "\n",
    "df1.write.save(\"data/ouput1\",format=\"json\")\n",
    "df1.write.partitionBy(\"name\").format(\"json\").save(\"data/ouput2\")\n",
    "df1.write.json(\"data/ouput3\")\n",
    "# 写入到持久化表\n",
    "# df1.write.partitionBy(\"favorite_color\")\\\n",
    "#     .bucketBy(42,\"name\")\\\n",
    "#     .saveAsTable(\"people_partitioned_bucketed\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 读取json的数据，写入到parquet\n",
    "\n",
    "dfj = spark.read.load(\"data/people.json\", format=\"json\")\n",
    "dfj.select(\"name\", \"age\")\\\n",
    "    .write\\\n",
    "    .save(\"data/namesAndAges.parquet\", format=\"parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 读取csv的数据\n",
    "\n",
    "dfc = spark.read.load(\"data/people.csv\",\n",
    "                      format=\"csv\", \n",
    "                      sep=\":\", \n",
    "                      inferSchema=\"true\", \n",
    "                      header=\"true\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 读取orc的数据，写入到orc\n",
    "\n",
    "dfo = spark.read.orc(\"data/users.orc\")\n",
    "dfo.write.format(\"orc\")\\\n",
    "    .option(\"orc.bloom.filter.columns\", \"favorite_color\")\\\n",
    "    .option(\"orc.dictionary.key.threshold\", \"1.0\")\\\n",
    "    .save(\"data/users_with_options.orc\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\soft\\python3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\soft\\python3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o149.load.\n: org.apache.spark.sql.AnalysisException: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".;\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:194)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fd31c975fcb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#读取avro\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdfa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"avro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/users.avro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdfa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"favorite_color\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"avro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/namesAndFavColors.avro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\soft\\python3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\soft\\python3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\soft\\python3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".;'"
     ],
     "ename": "AnalysisException",
     "evalue": "'Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of \"Apache Avro Data Source Guide\".;'",
     "output_type": "error"
    }
   ],
   "source": [
    "# 读取avro的数据，写入到avro\n",
    "# 集群执行：`spark-submit avro_read_sparksql.py  --master spark://zgg:7077 --packages org.apache.spark:spark-avro_2.12:2.4.4`\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"datasource\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "dfa = spark.read.format(\"avro\").load(\"/root/data/users.avro\")\n",
    "dfa.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"/root/data/namesAndFavColors.avro\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 根据 parquet file 创建临时视图\n",
    "\n",
    "def parquet_example(spark):\n",
    "    peopleDF = spark.read.json(\"data/people.json\")\n",
    "    peopleDF.write.parquet(\"data/people.parquet\")\n",
    "    \n",
    "    parquetFile = spark.read.parquet(\"data/people.parquet\")\n",
    "    \n",
    "    parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "    \n",
    "    spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\").show()\n",
    "\n",
    "parquet_example(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# 合并 parquet schema\n",
    "\n",
    "from pyspark.sql import Row\n",
    "def parquet_schema_merging_example(spark):\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    squaresDF = spark.createDataFrame(sc.parallelize(range(1,6))\n",
    "                                      .map(lambda x:Row(single=x,double=x**2)))\n",
    "    squaresDF.write.parquet(\"data/test_table/key=1\")\n",
    "    \n",
    "    cubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n",
    "                                    .map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "    cubesDF.write.parquet(\"data/test_table/key=2\")\n",
    "    \n",
    "    mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\n",
    "    mergedDF.printSchema()\n",
    "    mergedDF.show()\n",
    "\n",
    "parquet_schema_merging_example(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 读取json文件的另一种形式 RDD\n",
    "\n",
    "def json_dataset_example(spark):\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    '''\n",
    "    path：string represents path to the JSON dataset,or RDD of Strings storing JSON objects.\n",
    "    '''\n",
    "\n",
    "    path = \"data/people.json\"\n",
    "    peopleDF = spark.read.json(path)\n",
    "    peopleDF.printSchema()\n",
    "    \n",
    "    peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "    teenagerNamesDF.show()\n",
    "\n",
    "    # Alternatively, a DataFrame can be created for a JSON dataset represented by\n",
    "    # an RDD[String] storing one JSON object per string\n",
    "    jsonStrings = ['{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\n",
    "    otherPeopleRDD = sc.parallelize(jsonStrings)\n",
    "    otherPeople = spark.read.json(otherPeopleRDD)\n",
    "    otherPeople.show()\n",
    "\n",
    "json_dataset_example(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 连接 JDBC source\n",
    "\n",
    "要将 mysql-connector-java-8.0.21.jar 放到 spark 的 jars 目录下\n",
    "\n",
    "集群执行：`spark-submit mysql_integration__sparksql_test.py  --master spark://zgg:7077`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test_spark_mysql.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"datasource\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "def jdbc_dataset_example(spark):\n",
    "    # Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods\n",
    "    # Loading data from a JDBC source\n",
    "    jdbcDF = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://localhost:3306\") \\\n",
    "        .option(\"dbtable\", \"mysql.dept_emp\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"1234\") \\\n",
    "        .load()\n",
    "\n",
    "    jdbcDF.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://localhost:3306\") \\\n",
    "        .option(\"dbtable\", \"mysql.dept_emp_bk\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"1234\") \\\n",
    "        .save()\n",
    "\n",
    "jdbc_dataset_example(spark)  \n",
    "\n",
    "# 更多连接方式见 examples/src/main/python/sql/datasource.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 连接 hive\n",
    "\n",
    "1、在 classpath 添加许多依赖，将 hive-site.xml, core-site.xml 和 hdfs-site.xml 放到 conf/. 目录下。\n",
    "\n",
    "2、实例化一个 Hive 支持的 SparkSession\n",
    "\n",
    "3、集群执行：`spark-submit hive_integration_sparksql_test.py  --master spark://zgg:7077 >hive_test_spark.log`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def test_hive_example(spark):\n",
    "    spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")\n",
    "    spark.sql(\"LOAD DATA LOCAL INPATH '/root/data/kv1.txt' INTO TABLE src\")\n",
    "    \n",
    "    print(\"SHOW ALL DATA:\")\n",
    "    spark.sql(\"SELECT * FROM src\").show()\n",
    "    \n",
    "    print(\"AGGREGATION QUERIES:\")\n",
    "# Aggregation queries are also supported.\n",
    "    spark.sql(\"SELECT COUNT(*) FROM src\").show()\n",
    "\n",
    "    print(\"WHERE FILTER DATA:\")\n",
    "# The results of SQL queries are themselves DataFrames and support all normal functions.\n",
    "    sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\")\n",
    "\n",
    "    print(\"TRANSFER TO RDD:\")\n",
    "# The items in DataFrames are of type Row, which allows you to access each column by ordinal.\n",
    "    stringsDS = sqlDF.rdd.map(lambda row: \"Key: %d, Value: %s\" % (row.key, row.value))\n",
    "    for record in stringsDS.collect():\n",
    "        print(record)\n",
    "        \n",
    "# You can also use DataFrames to create temporary views within a SparkSession.\n",
    "    Record = Row(\"key\", \"value\")\n",
    "    recordsDF = spark.createDataFrame([Record(i, \"val_\" + str(i)) for i in range(1, 101)])\n",
    "    recordsDF.createOrReplaceTempView(\"records\")\n",
    "\n",
    "    print(\"CREATE TEMP VIEW:\")\n",
    "# Queries can then join DataFrame data with data stored in Hive.\n",
    "    spark.sql(\"SELECT * FROM records r JOIN src s ON r.key = s.key\").show()\n",
    "    \n",
    "test_hive_example(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 一些通用配置\n",
    "\n",
    "当前版本：pyspark==2.4.4，下面内容在spark3.0.0下运行"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 忽略损坏文件\n",
    "\n",
    "spark.sql(\"set spark.sql.files.ignoreCorruptFiles=true\")\n",
    "# dir1/file3.json is corrupt from parquet's view\n",
    "test_corrupt_df = spark.read.parquet(\"data/dir1\",\"data/dir1/dir2\")\n",
    "test_corrupt_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 忽略缺失文件\n",
    "\n",
    "spark.sql(\"set spark.sql.files.ignoreMissingFiles=true\")\n",
    "test_corrupt_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 路径全局过滤\n",
    "\n",
    "spark.sql(\"set spark.sql.files.ignoreCorruptFiles=false\")\n",
    "test_filter_df = spark.read.load(\"data/dir1\",format=\"parquet\",pathGlobFilter=\"*.parquet\")\n",
    "test_filter_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 递归查找文件\n",
    "\n",
    "spark.sql(\"set spark.sql.files.ignoreCorruptFiles=true\")\n",
    "recursive_loaded_df = spark.read.format(\"parquet\")\\\n",
    "    .option(\"recursiveFileLookup\",\"true\")\\\n",
    "    .load(\"data/dir1\")\n",
    "recursive_loaded_df.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}