{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "'''\n",
    "来自：examples/src/main/python/sql/datasource.py\n",
    "包含：\n",
    "    1、读取、写入数据（三种方式）\n",
    "    2、根据 parquet file 、json file 等创建临时视图\n",
    "    3、合并 parquet schema\n",
    "    3、连接 mysql\n",
    "                    \n",
    "DataFrame函数：http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "'''\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"datasource\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n\n+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n\n",
      "+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 三种读取方式\n",
    "\n",
    "df1 = spark.read.load(\"data/users.parquet\")\n",
    "df2 = spark.read.parquet(\"data/users.parquet\")\n",
    "df3 = spark.sql(\"select * from parquet.`data/users.parquet`\")\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "df3.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 三种写入方式  pyspark==2.4.4\n",
    "\n",
    "df1.write.save(\"data/ouput1\",format=\"json\")\n",
    "df1.write.partitionBy(\"name\").format(\"json\").save(\"data/ouput2\")\n",
    "df1.write.json(\"data/ouput3\")\n",
    "# 写入到持久化表\n",
    "# df1.write.partitionBy(\"favorite_color\")\\\n",
    "#     .bucketBy(42,\"name\")\\\n",
    "#     .saveAsTable(\"people_partitioned_bucketed\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 读取其他格式的文件\n",
    "\n",
    "dfj = spark.read.load(\"data/people.json\", format=\"json\")\n",
    "dfj.select(\"name\", \"age\")\\\n",
    "    .write\\\n",
    "    .save(\"namesAndAges.parquet\", format=\"parquet\")\n",
    "\n",
    "dfc = spark.read.load(\"data/people.csv\",\n",
    "                      format=\"csv\", \n",
    "                      sep=\":\", \n",
    "                      inferSchema=\"true\", \n",
    "                      header=\"true\")\n",
    "\n",
    "dfo = spark.read.orc(\"data/users.orc\")\n",
    "dfo.write.format(\"orc\")\\\n",
    "    .option(\"orc.bloom.filter.columns\", \"favorite_color\")\\\n",
    "    .option(\"orc.dictionary.key.threshold\", \"1.0\")\\\n",
    "    .save(\"users_with_options.orc\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+------+\n|  name|\n+------+\n|Justin|\n+------+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 根据 parquet file 创建临时视图\n",
    "\n",
    "def parquet_example(spark):\n",
    "    peopleDF = spark.read.json(\"data/people.json\")\n",
    "    peopleDF.write.parquet(\"data/people.parquet\")\n",
    "    \n",
    "    parquetFile = spark.read.parquet(\"data/people.parquet\")\n",
    "    \n",
    "    parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "    \n",
    "    spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\").show()\n",
    "\n",
    "parquet_example(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "root\n |-- double: long (nullable = true)\n |-- single: long (nullable = true)\n |-- triple: long (nullable = true)\n |-- key: integer (nullable = true)\n\n+------+------+------+---+\n|double|single|triple|key|\n+------+------+------+---+\n|  null|     6|   216|  2|\n|  null|     7|   343|  2|\n|  null|     8|   512|  2|\n|  null|     9|   729|  2|\n|  null|    10|  1000|  2|\n|     1|     1|  null|  1|\n|     4|     2|  null|  1|\n|     9|     3|  null|  1|\n|    16|     4|  null|  1|\n|    25|     5|  null|  1|\n+------+------+------+---+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 合并 parquet schema\n",
    "\n",
    "from pyspark.sql import Row\n",
    "def parquet_schema_merging_example(spark):\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    squaresDF = spark.createDataFrame(sc.parallelize(range(1,6))\n",
    "                                      .map(lambda x:Row(single=x,double=x**2)))\n",
    "    squaresDF.write.parquet(\"data/test_table/key=1\")\n",
    "    \n",
    "    cubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n",
    "                                    .map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "    cubesDF.write.parquet(\"data/test_table/key=2\")\n",
    "    \n",
    "    mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\n",
    "    mergedDF.printSchema()\n",
    "    mergedDF.show()\n",
    "\n",
    "parquet_schema_merging_example(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 读取json文件的另一种形式 RDD\n",
    "\n",
    "def json_dataset_example(spark):\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    '''\n",
    "    path：string represents path to the JSON dataset,or RDD of Strings storing JSON objects.\n",
    "    '''\n",
    "\n",
    "    path = \"data/people.json\"\n",
    "    peopleDF = spark.read.json(path)\n",
    "    peopleDF.printSchema()\n",
    "    \n",
    "    peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "    teenagerNamesDF.show()\n",
    "\n",
    "    # Alternatively, a DataFrame can be created for a JSON dataset represented by\n",
    "    # an RDD[String] storing one JSON object per string\n",
    "    jsonStrings = ['{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\n",
    "    otherPeopleRDD = sc.parallelize(jsonStrings)\n",
    "    otherPeople = spark.read.json(otherPeopleRDD)\n",
    "    otherPeople.show()\n",
    "\n",
    "json_dataset_example(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6a354867ffd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mjdbcDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mjdbc_dataset_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-6a354867ffd8>\u001b[0m in \u001b[0;36mjdbc_dataset_example\u001b[1;34m(spark)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dbtable\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"mysql.dept_emp\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"user\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"root\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"password\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"1234\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\soft\\python3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\soft\\python3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\soft\\python3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\soft\\python3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1019.load.\n: java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ],
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1019.load.\n: java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error"
    }
   ],
   "source": [
    "'''\n",
    "连接 JDBC source\n",
    "\n",
    "要将 mysql-connector-java-8.0.21.jar 放到 spark 的 jars 目录下\n",
    "\n",
    "集群执行：spark-submit test.py --master spark://zgg:7077\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"datasource\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "def jdbc_dataset_example(spark):\n",
    "    # Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods\n",
    "    # Loading data from a JDBC source\n",
    "    jdbcDF = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://localhost:3306\") \\\n",
    "        .option(\"dbtable\", \"mysql.dept_emp\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"1234\") \\\n",
    "        .load()\n",
    "\n",
    "    jdbcDF.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://localhost:3306\") \\\n",
    "        .option(\"dbtable\", \"mysql.dept_emp_bk\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"1234\") \\\n",
    "        .save()\n",
    "\n",
    "jdbc_dataset_example(spark)  \n",
    "\n",
    "'''\n",
    "    jdbcDF2 = spark.read \\\n",
    "        .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "              properties={\"user\": \"username\", \"password\": \"password\"})\n",
    "\n",
    "    # Specifying dataframe column data types on read\n",
    "    jdbcDF3 = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "        .option(\"dbtable\", \"schema.tablename\") \\\n",
    "        .option(\"user\", \"username\") \\\n",
    "        .option(\"password\", \"password\") \\\n",
    "        .option(\"customSchema\", \"id DECIMAL(38, 0), name STRING\") \\\n",
    "        .load()\n",
    "        \n",
    "    jdbcDF2.write \\\n",
    "        .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "              properties={\"user\": \"username\", \"password\": \"password\"})\n",
    "\n",
    "    # Specifying create table column data types on write\n",
    "    jdbcDF.write \\\n",
    "        .option(\"createTableColumnTypes\", \"name CHAR(64), comments VARCHAR(1024)\") \\\n",
    "        .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "              properties={\"user\": \"username\", \"password\": \"password\"})\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}